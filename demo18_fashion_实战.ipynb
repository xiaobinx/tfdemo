{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e176da0a-8471-491c-9151-f5487462895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, datasets, layers, metrics, optimizers\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9e904b-ccf0-482a-9bb3-a8f51374b657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "x.shape, y.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f06c45-cbb2-414b-9066-b50d569d7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477222b7-250b-4a3c-81e7-ddf304f6ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.0\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2227614c-450c-4413-a08e-b2c7233dcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess).shuffle(10000).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf26200e-e839-4f33-83c0-a90d3f1fb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        layers.Dense(256, activation=tf.nn.relu),\n",
    "        layers.Dense(128, activation=tf.nn.relu),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(32, activation=tf.nn.relu),\n",
    "        layers.Dense(10, activation=tf.nn.relu),\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=[None, 28 * 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838d74bb-ea79-4935-8786-f9be4940e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7435b73-9dfe-4afa-bb81-87d1945c49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch: int):\n",
    "    loss_sum = 0\n",
    "    x_count = 0\n",
    "    for step, (x, y) in enumerate(db):\n",
    "        x = tf.reshape(x, [-1, 28 * 28])\n",
    "        x_count += x.shape[0] # display element\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss_sum += float(tf.reduce_sum(loss_ce)) # display element\n",
    "            loss_ce = tf.reduce_mean(loss_ce)\n",
    "        grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        loss = float(loss_ce)\n",
    "        # if step % 100 == 0:\n",
    "        #     print(f\"[{epoch}]({step})> loss_ce: {float(loss_ce)}.\")\n",
    "\n",
    "    loss = loss_sum / x_count\n",
    "    ## test\n",
    "    total_corrent = 0\n",
    "    total_num = 0\n",
    "    for x, y in db_test:\n",
    "        x = tf.reshape(x, [-1, 28 * 28])\n",
    "        logits = model(x)  # [b, 10]\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)  # the index of max value in prob's axis=1 => [b]\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        corrent = tf.equal(pred, y)  # [b], True: equal, False: not equal\n",
    "        corrent = tf.reduce_sum(tf.cast(corrent, dtype=tf.int32))\n",
    "\n",
    "        total_corrent += int(corrent)\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = round(total_corrent / total_num, 4)\n",
    "    \n",
    "    print(f\"[{epoch}]> test acc: {acc}, x_count: {x_count}, loss: {loss}\")\n",
    "\n",
    "\n",
    "def train(epochs: int):\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "454949bc-7ecc-4dfc-92a3-b7096a18634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0](0)> loss_ce: 0.4720242917537689.\n",
      "[0](100)> loss_ce: 0.3571893572807312.\n",
      "[0](200)> loss_ce: 0.496612012386322.\n",
      "[0](300)> loss_ce: 0.46405869722366333.\n",
      "[0](400)> loss_ce: 0.33381739258766174.\n",
      "[0]> test acc: 0.8814, x_count: 60000, loss: 0.3920767405192057\n",
      "[1](0)> loss_ce: 0.4182037115097046.\n",
      "[1](100)> loss_ce: 0.3833233416080475.\n",
      "[1](200)> loss_ce: 0.2686905860900879.\n",
      "[1](300)> loss_ce: 0.4610295295715332.\n",
      "[1](400)> loss_ce: 0.44221892952919006.\n",
      "[1]> test acc: 0.871, x_count: 60000, loss: 0.38620709657669067\n",
      "[2](0)> loss_ce: 0.4430846571922302.\n",
      "[2](100)> loss_ce: 0.386136919260025.\n",
      "[2](200)> loss_ce: 0.46806713938713074.\n",
      "[2](300)> loss_ce: 0.3248709738254547.\n",
      "[2](400)> loss_ce: 0.39974498748779297.\n",
      "[2]> test acc: 0.8709, x_count: 60000, loss: 0.37882201042175295\n",
      "[3](0)> loss_ce: 0.27965718507766724.\n",
      "[3](100)> loss_ce: 0.2871922552585602.\n",
      "[3](200)> loss_ce: 0.3779420554637909.\n",
      "[3](300)> loss_ce: 0.39333266019821167.\n",
      "[3](400)> loss_ce: 0.4212396740913391.\n",
      "[3]> test acc: 0.8903, x_count: 60000, loss: 0.3813343811988831\n",
      "[4](0)> loss_ce: 0.3948444128036499.\n",
      "[4](100)> loss_ce: 0.31803855299949646.\n",
      "[4](200)> loss_ce: 0.30887648463249207.\n",
      "[4](300)> loss_ce: 0.280875027179718.\n",
      "[4](400)> loss_ce: 0.2637677490711212.\n",
      "[4]> test acc: 0.8886, x_count: 60000, loss: 0.3731349102020264\n"
     ]
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70208211-26b9-49c9-bdd0-34a776979cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
